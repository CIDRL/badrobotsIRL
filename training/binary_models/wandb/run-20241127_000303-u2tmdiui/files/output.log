{'activation_function': 'relu', 'batch_size': 64, 'dense_units': 64, 'dropout_rate': 0.5, 'epochs': 500, 'fusion_type': 'intermediate', 'gru_units': 128, 'learning_rate': 0.01, 'loss': 'categorical_crossentropy', 'modality': 'combined', 'num_gru_layers': 1, 'optimizer': 'adadelta', 'recurrent_regularizer': 'l1_l2', 'sequence_length': 30, 'use_bidirectional': False, 'use_norm': True, 'use_pca': False, 'use_stats': True}
