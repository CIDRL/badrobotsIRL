{'activation_function': 'relu', 'batch_size': 64, 'dense_units': 64, 'dropout_rate': 0.8, 'epochs': 500, 'fusion_type': 'intermediate', 'gru_units': 128, 'learning_rate': 0.001, 'loss': 'categorical_crossentropy', 'modality': 'facial', 'num_gru_layers': 3, 'optimizer': 'adadelta', 'recurrent_regularizer': 'l1_l2', 'sequence_length': 30, 'use_bidirectional': False, 'use_norm': False, 'use_pca': True, 'use_stats': True}
